{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYYowkluVt1R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 1) Paths --------\n",
        "DATA_DIR = Path(\"/content/\")  # <- change this to your actual data directory\n",
        "files = {\n",
        "    \"customer_train\": DATA_DIR / \"customer_nodes_training.csv\",\n",
        "    \"product_train\":  DATA_DIR / \"product_nodes_training.csv\",\n",
        "    \"events_train\":   DATA_DIR / \"event_table_training.csv\",\n",
        "    \"customer_test\":  DATA_DIR / \"customer_nodes_testing.csv\",\n",
        "    \"product_test\":   DATA_DIR / \"product_nodes_testing.csv\",\n",
        "    \"events_test\":    DATA_DIR / \"event_table_testing.csv\",\n",
        "}"
      ],
      "metadata": {
        "id": "ofr0slqICx_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 2) Load CSVs --------\n",
        "df_customers = pd.read_csv(files[\"customer_train\"])\n",
        "df_products  = pd.read_csv(files[\"product_train\"])\n",
        "df_events    = pd.read_csv(files[\"events_train\"])\n",
        "\n",
        "print(\"Shapes (customers, products, events):\", df_customers.shape, df_products.shape, df_events.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvU6V8-DC1S7",
        "outputId": "7eb05d98-96b9-4b13-944b-6e9b4720d1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes (customers, products, events): (443343, 30) (44165, 44) (1369133, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 3) Clean column names --------\n",
        "def clean_col_names(df):\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "df_customers = clean_col_names(df_customers)\n",
        "df_products = clean_col_names(df_products)\n",
        "df_events = clean_col_names(df_events)"
      ],
      "metadata": {
        "id": "O1b7NfV2C47C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 4) Identify label column --------\n",
        "label_col = None\n",
        "if 'isreturned' in df_events.columns:\n",
        "    label_col = 'isreturned'\n",
        "else:\n",
        "    possible_label_names = [\"return\", \"is_return\", \"label\", \"returned\"]\n",
        "    for col in df_events.columns:\n",
        "        if col.strip().lower() in possible_label_names:\n",
        "            label_col = col\n",
        "            break\n",
        "\n",
        "if label_col is None:\n",
        "    raise ValueError(\"No label column found; check event table column names.\")\n",
        "\n",
        "print(f\"Label column: {label_col}\")\n",
        "print(\"Label distribution (train):\")\n",
        "print(df_events[label_col].value_counts(normalize=False))\n",
        "print(df_events[label_col].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQmjLfe3C7pI",
        "outputId": "c072168d-208f-4c28-fae5-831fb720df9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label column: isreturned\n",
            "Label distribution (train):\n",
            "isreturned\n",
            "1    757227\n",
            "0    611906\n",
            "Name: count, dtype: int64\n",
            "isreturned\n",
            "1    0.55307\n",
            "0    0.44693\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 5) Suggest ID columns --------\n",
        "def suggest_id_columns(df, label):\n",
        "    print(f\"\\nðŸ”Ž Inspecting columns in {label} DataFrame:\")\n",
        "    cols = df.columns.tolist()\n",
        "    print(\"Columns:\", cols)\n",
        "\n",
        "    cust_candidates = [c for c in cols if any(k in c.lower() for k in [\"cust\", \"customer\"])]\n",
        "    prod_candidates = [c for c in cols if any(k in c.lower() for k in [\"prod\", \"product\", \"variant\"])]\n",
        "    print(\"Suggested customer ID columns:\", cust_candidates)\n",
        "    print(\"Suggested product ID columns:\", prod_candidates)\n",
        "\n",
        "    return cust_candidates, prod_candidates\n",
        "\n",
        "cust_event_candidates, prod_event_candidates = suggest_id_columns(df_events, \"Events\")\n",
        "cust_node_candidates, _ = suggest_id_columns(df_customers, \"Customers\")\n",
        "_, prod_node_candidates = suggest_id_columns(df_products, \"Products\")\n",
        "\n",
        "cust_id_col_event = cust_event_candidates[0] if cust_event_candidates else None\n",
        "prod_id_col_event = prod_event_candidates[0] if prod_event_candidates else None\n",
        "cust_id_col_node = cust_node_candidates[0] if cust_node_candidates else None\n",
        "prod_id_col_node = prod_node_candidates[0] if prod_node_candidates else None\n",
        "\n",
        "if not all([cust_id_col_event, prod_id_col_event, cust_id_col_node, prod_id_col_node]):\n",
        "    raise ValueError(\"âŒ Could not identify customer or product ID columns for merging. Please check the printed suggestions above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egCxbkdUC-A5",
        "outputId": "506143fd-a9f4-4a9f-e590-0d0212c0bef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”Ž Inspecting columns in Events DataFrame:\n",
            "Columns: ['hash(variantid)', 'hash(customerid)', 'isreturned']\n",
            "Suggested customer ID columns: ['hash(customerid)']\n",
            "Suggested product ID columns: ['hash(variantid)']\n",
            "\n",
            "ðŸ”Ž Inspecting columns in Customers DataFrame:\n",
            "Columns: ['hash(customerid)', 'yearofbirth', 'ismale', 'shippingcountry', 'premier', 'salespercustomer', 'returnspercustomer', 'customerreturnrate', 'customerid_level_return_code_a', 'customerid_level_return_code_b', 'customerid_level_return_code_c', 'customerid_level_return_code_d', 'customerid_level_return_code_e', 'customerid_level_return_code_d.1', 'customerid_level_return_code_f', 'customerid_level_return_code_g', 'customerid_level_return_code_h', 'customerid_level_return_code_i', 'customerid_level_return_code_j', 'customerid_level_return_code_k', 'customerid_level_return_code_l', 'country_a', 'country_b', 'country_c', 'country_d', 'country_e', 'country_f', 'country_g', 'country_h', 'country_i']\n",
            "Suggested customer ID columns: ['hash(customerid)', 'salespercustomer', 'returnspercustomer', 'customerreturnrate', 'customerid_level_return_code_a', 'customerid_level_return_code_b', 'customerid_level_return_code_c', 'customerid_level_return_code_d', 'customerid_level_return_code_e', 'customerid_level_return_code_d.1', 'customerid_level_return_code_f', 'customerid_level_return_code_g', 'customerid_level_return_code_h', 'customerid_level_return_code_i', 'customerid_level_return_code_j', 'customerid_level_return_code_k', 'customerid_level_return_code_l']\n",
            "Suggested product ID columns: []\n",
            "\n",
            "ðŸ”Ž Inspecting columns in Products DataFrame:\n",
            "Columns: ['hash(variantid)', 'hash(productid)', 'producttype', 'hash(supplierref)', 'branddesc', 'avggbpprice', 'avgdiscountvalue', 'salesperproduct', 'returnsperproduct', 'productreturnrate', 'variantid_level_return_code_a', 'variantid_level_return_code_b', 'variantid_level_return_code_c', 'variantid_level_return_code_d', 'variantid_level_return_code_e', 'variantid_level_return_code_d.1', 'variantid_level_return_code_f', 'variantid_level_return_code_g', 'variantid_level_return_code_h', 'variantid_level_return_code_i', 'variantid_level_return_code_j', 'variantid_level_return_code_k', 'variantid_level_return_code_l', 'brand_a', 'brand_b', 'brand_c', 'brand_d', 'brand_e', 'brand_f', 'brand_g', 'brand_i', 'brand_j', 'brand_k', 'producttype_a', 'producttype_b', 'producttype_c', 'producttype_d', 'producttype_e', 'producttype_f', 'producttype_g', 'producttype_h', 'producttype_i', 'producttype_j', 'producttype_k']\n",
            "Suggested customer ID columns: []\n",
            "Suggested product ID columns: ['hash(variantid)', 'hash(productid)', 'producttype', 'salesperproduct', 'returnsperproduct', 'productreturnrate', 'variantid_level_return_code_a', 'variantid_level_return_code_b', 'variantid_level_return_code_c', 'variantid_level_return_code_d', 'variantid_level_return_code_e', 'variantid_level_return_code_d.1', 'variantid_level_return_code_f', 'variantid_level_return_code_g', 'variantid_level_return_code_h', 'variantid_level_return_code_i', 'variantid_level_return_code_j', 'variantid_level_return_code_k', 'variantid_level_return_code_l', 'producttype_a', 'producttype_b', 'producttype_c', 'producttype_d', 'producttype_e', 'producttype_f', 'producttype_g', 'producttype_h', 'producttype_i', 'producttype_j', 'producttype_k']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 6) Standardize ID column names --------\n",
        "if cust_id_col_node != \"customer_id\":\n",
        "    df_customers = df_customers.rename(columns={cust_id_col_node: \"customer_id\"})\n",
        "    cust_id_col_node = \"customer_id\"\n",
        "\n",
        "if prod_id_col_node != \"product_id\":\n",
        "    df_products = df_products.rename(columns={prod_id_col_node: \"product_id\"})\n",
        "    prod_id_col_node = \"product_id\""
      ],
      "metadata": {
        "id": "YJaYIKSrC-GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 7) Merge datasets --------\n",
        "df = df_events.merge(df_customers, left_on=cust_id_col_event, right_on=cust_id_col_node, how=\"left\", suffixes=(\"\", \"_cust\"))\n",
        "df = df.merge(df_products, left_on=prod_id_col_event, right_on=prod_id_col_node, how=\"left\", suffixes=(\"\", \"_prod\"))\n",
        "\n",
        "print(\"Merged dataset shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lWTZCJbDF6I",
        "outputId": "cb458afe-b514-4405-f1c4-2ef6c9e071ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged dataset shape: (1369133, 77)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 8) Missingness report --------\n",
        "def missing_report(df_in):\n",
        "    miss = df_in.isna().mean().sort_values(ascending=False)\n",
        "    return miss[miss > 0]\n",
        "\n",
        "print(\"Missingness top columns:\")\n",
        "print(missing_report(df).head(20))\n",
        "\n",
        "high_missing_thresh = 0.80\n",
        "cols_to_drop = missing_report(df)[missing_report(df) > high_missing_thresh].index.tolist()\n",
        "print(\"Dropping cols with >80% missing:\", cols_to_drop)\n",
        "df = df.drop(columns=cols_to_drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4n-XIQzDGAQ",
        "outputId": "d9272eec-5293-4335-e872-48f14b96b4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missingness top columns:\n",
            "producttype_g                    0.930414\n",
            "producttype_h                    0.930414\n",
            "producttype_a                    0.930414\n",
            "producttype_b                    0.930414\n",
            "producttype_c                    0.930414\n",
            "producttype_j                    0.930414\n",
            "producttype_i                    0.930414\n",
            "producttype_k                    0.930414\n",
            "brand_k                          0.930414\n",
            "brand_j                          0.930414\n",
            "brand_i                          0.930414\n",
            "brand_g                          0.930414\n",
            "brand_f                          0.930414\n",
            "variantid_level_return_code_j    0.930414\n",
            "variantid_level_return_code_k    0.930414\n",
            "variantid_level_return_code_l    0.930414\n",
            "brand_a                          0.930414\n",
            "brand_b                          0.930414\n",
            "brand_e                          0.930414\n",
            "brand_c                          0.930414\n",
            "dtype: float64\n",
            "Dropping cols with >80% missing: ['producttype_g', 'producttype_h', 'producttype_a', 'producttype_b', 'producttype_c', 'producttype_j', 'producttype_i', 'producttype_k', 'brand_k', 'brand_j', 'brand_i', 'brand_g', 'brand_f', 'variantid_level_return_code_j', 'variantid_level_return_code_k', 'variantid_level_return_code_l', 'brand_a', 'brand_b', 'brand_e', 'brand_c', 'brand_d', 'variantid_level_return_code_i', 'variantid_level_return_code_g', 'variantid_level_return_code_h', 'producttype_d', 'producttype_e', 'producttype_f', 'variantid_level_return_code_b', 'variantid_level_return_code_c', 'returnsperproduct', 'productreturnrate', 'variantid_level_return_code_a', 'variantid_level_return_code_d.1', 'hash(supplierref)', 'producttype', 'hash(productid)', 'product_id', 'variantid_level_return_code_f', 'variantid_level_return_code_e', 'variantid_level_return_code_d', 'branddesc', 'avgdiscountvalue', 'avggbpprice', 'salesperproduct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 9) Feature types --------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if label_col in numeric_cols:\n",
        "    numeric_cols.remove(label_col)\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "ids_to_remove_from_cat = [cust_id_col_event, prod_id_col_event, 'customer_id', 'product_id']\n",
        "categorical_cols = [c for c in categorical_cols if c not in ids_to_remove_from_cat]\n",
        "\n",
        "print(\"Numeric:\", len(numeric_cols), \"Categorical:\", len(categorical_cols))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kfiEQcLDGEI",
        "outputId": "139cb2ff-2a69-4b64-bc8f-009874b38626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric: 31 Categorical: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 10) Preprocessing pipelines --------\n",
        "num_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # âœ… updated here\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, numeric_cols),\n",
        "    (\"cat\", cat_pipeline, categorical_cols)\n",
        "], remainder=\"drop\", sparse_threshold=0)\n",
        "\n",
        "X = df.drop(columns=[label_col])\n",
        "y = df[label_col].astype(int)\n",
        "\n",
        "print(\"Fitting preprocessor...\")\n",
        "preprocessor.fit(X)\n",
        "X_trans = preprocessor.transform(X)\n",
        "print(\"Transformed X shape:\", X_trans.shape)\n",
        "\n",
        "joblib.dump(preprocessor, \"preprocessor_train.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9f2xSQHDMMI",
        "outputId": "091001bb-593b-4aa2-e2e2-776b638a76ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting preprocessor...\n",
            "Transformed X shape: (1369133, 41)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['preprocessor_train.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 11) Train/validation split --------\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trans, y, test_size=0.1, stratify=y, random_state=42)\n",
        "print(\"Train/val sizes:\", X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpA2Hyc1DMTT",
        "outputId": "bf77cfaf-ff66-4bec-c443-aa22bb14ee56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/val sizes: (1232219, 41) (136914, 41) (1232219,) (136914,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 12) Build bipartite graph --------\n",
        "G = nx.Graph()\n",
        "\n",
        "cust_feat_cols = [c for c in df_customers.columns if c != \"customer_id\"]\n",
        "for _, row in df_customers.iterrows():\n",
        "    node_id = f\"c_{row['customer_id']}\"\n",
        "    if pd.notna(row['customer_id']):\n",
        "        G.add_node(node_id, bipartite=0, **{k: row[k] for k in cust_feat_cols if pd.notna(row[k])})\n",
        "\n",
        "prod_feat_cols = [c for c in df_products.columns if c != \"product_id\"]\n",
        "for _, row in df_products.iterrows():\n",
        "    node_id = f\"p_{row['product_id']}\"\n",
        "    if pd.notna(row['product_id']):\n",
        "        G.add_node(node_id, bipartite=1, **{k: row[k] for k in prod_feat_cols if pd.notna(row[k])})\n",
        "\n",
        "for _, r in df_events.iterrows():\n",
        "    c_id_event = r[cust_id_col_event]\n",
        "    p_id_event = r[prod_id_col_event]\n",
        "    label = int(r[label_col])\n",
        "\n",
        "    c_node_id = f\"c_{c_id_event}\"\n",
        "    p_node_id = f\"p_{p_id_event}\"\n",
        "\n",
        "    if G.has_node(c_node_id) and G.has_node(p_node_id):\n",
        "        G.add_edge(c_node_id, p_node_id, label=label)\n",
        "\n",
        "print(\"Graph nodes/edges:\", G.number_of_nodes(), G.number_of_edges())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGL58oGbDMWw",
        "outputId": "d2732840-44fa-45cb-bbc7-428f06c5867d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph nodes/edges: 487508 50275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 13) Save processed datasets --------\n",
        "joblib.dump((X_train, X_val, y_train, y_val), \"tabular_train_val.joblib\")\n",
        "joblib.dump(df, \"merged_events_train.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSlLvEWVDbFe",
        "outputId": "d5c1800e-42d3-40d1-f363-fe109831d5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['merged_events_train.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de19e81f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "*   **What are the key steps of the analysis performed in the notebook?**\n",
        "    The analysis involved loading three datasets (customer, product, and event data), cleaning column names, identifying and standardizing ID columns, merging the datasets, handling missing values (dropping columns with >80% missing data, imputing remaining numerical and categorical features), identifying feature types, applying preprocessing pipelines (StandardScaler for numerical, OneHotEncoder for categorical), splitting the data into training and validation sets (stratified 90/10 split), and building a bipartite graph connecting customers and products based on events.\n",
        "*   **What are the potential errors or limitations of the current analysis?**\n",
        "    Potential issues include limitations of the chosen imputation strategies (median for numerical, constant for categorical), potential impact of One-Hot Encoding on feature dimensionality if applied to high-cardinality features, the `handle_unknown='ignore'` setting in OneHotEncoder which treats unknown categories as all zeros, the simplicity of the random train/validation split which might not capture temporal or group dependencies, the exclusion of customers/products from the graph if they are only in the event table but not the node files, and the exclusion of event-specific features as edge attributes in the graph.\n",
        "*   **What improvements are suggested for the analysis?**\n",
        "    Suggested improvements include exploring more sophisticated imputation methods (e.g., KNN, model-based), handling outliers, engineering new features, considering alternative categorical encoding methods, enriching the bipartite graph with more node and edge attributes, computing graph-based features or embeddings, exploring various modeling approaches (traditional ML, graph-enhanced models, GNNs), using more appropriate train/validation split strategies (temporal or group-based), and evaluating models using a wider range of metrics relevant to the business problem (Precision, Recall, F1-Score, AUC-ROC, cost-sensitive metrics).\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The analysis successfully loaded, cleaned, and merged customer, product, and event data into a single dataframe with a shape of (1369133, 77) before dropping columns.\n",
        "*   The target variable `isreturned` in the training event data is slightly imbalanced, with approximately 55.3% of events being returns (1) and 44.7% being non-returns (0).\n",
        "*   A significant number of columns (those with >80% missing values) were dropped from the merged dataframe during preprocessing.\n",
        "*   After imputation and feature transformation using StandardScaler and OneHotEncoder, the preprocessed feature matrix (`X_trans`) has a shape of (1369133, 41).\n",
        "*   The data was split into a training set of shape (1232219, 41) and a validation set of shape (136914, 41), maintaining the original label distribution through stratification.\n",
        "*   A bipartite graph was constructed representing customer-product interactions, containing 487508 nodes and 50275 edges.\n",
        "\n",
        "### Insights\n",
        "\n",
        "*   The current analysis provides a solid foundation of data preparation and basic feature engineering. The next crucial step is to train and evaluate predictive models using the prepared tabular data and explore methods to leverage the constructed bipartite graph for improved prediction performance.\n",
        "*   Given the potential limitations identified, future work should focus on implementing more advanced data preprocessing techniques, exploring sophisticated modeling approaches (including those that utilize the graph structure), and carefully selecting evaluation metrics that align with the business objectives of return prediction.\n"
      ]
    }
  ]
}
